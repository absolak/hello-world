{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['words\\\\english-word-list-total.xls', 'words\\\\french-word-list-total.xls', 'words\\\\german-word-list-total.xls', 'words\\\\italian-word-list-total.xls', 'words\\\\russian-word-list-total.xls', 'words\\\\spanish-word-list-total.xls']\n"
     ]
    }
   ],
   "source": [
    "def findFiles(path): return glob.glob(path)\n",
    "print(findFiles('words/*.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import ExcelWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = []\n",
    "df.append(pd.read_excel('words/english-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/french-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/german-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/italian-word-list-total.xls'))\n",
    "#df.append(pd.read_excel('words/russian-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/spanish-word-list-total.xls'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "de\n"
     ]
    }
   ],
   "source": [
    "print(df[4].iloc[2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    language:     French  Unnamed: 2\n",
      "1.0        de  484735124    1.000000\n",
      "2.0        la  260760851    0.537945\n",
      "3.0        et  224298880    0.462725\n",
      "4.0        le  213270164    0.439973\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    df[i] = df[i].iloc[2:,]\n",
    "\n",
    "print(df[1].head(4))\n",
    "#df = df.iloc[2:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "en = df[0].iloc[:,0]\n",
    "fr = df[1].iloc[:,0]\n",
    "ge = df[2].iloc[:,0]\n",
    "'''\n",
    "for i in range(5):\n",
    "    df[i] = df[i].iloc[:,0]\n",
    "    df[i] = df[i].iloc[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0                de\n",
      "2.0                la\n",
      "3.0               que\n",
      "4.0                el\n",
      "5.0                 y\n",
      "6.0                en\n",
      "7.0                 a\n",
      "8.0               los\n",
      "9.0               del\n",
      "10.0               se\n",
      "11.0              las\n",
      "12.0              por\n",
      "13.0               un\n",
      "14.0              con\n",
      "15.0             para\n",
      "16.0               no\n",
      "17.0              una\n",
      "18.0               es\n",
      "19.0               su\n",
      "20.0               al\n",
      "21.0               lo\n",
      "22.0             como\n",
      "23.0              más\n",
      "24.0                o\n",
      "25.0             este\n",
      "26.0             pero\n",
      "27.0              sus\n",
      "28.0             esta\n",
      "29.0               si\n",
      "30.0               ha\n",
      "             ...     \n",
      "471.0        análisis\n",
      "472.0            mano\n",
      "473.0         humanos\n",
      "474.0       instituto\n",
      "475.0        superior\n",
      "476.0          propio\n",
      "477.0           señor\n",
      "478.0           santa\n",
      "479.0           favor\n",
      "480.0       municipio\n",
      "481.0           cerca\n",
      "482.0          tierra\n",
      "483.0       políticas\n",
      "484.0       programas\n",
      "485.0        ambiente\n",
      "486.0     oportunidad\n",
      "487.0         domingo\n",
      "488.0        economía\n",
      "489.0          crisis\n",
      "490.0           marzo\n",
      "491.0         mejores\n",
      "492.0         interés\n",
      "493.0            etc.\n",
      "494.0    conocimiento\n",
      "495.0           sigue\n",
      "496.0       necesidad\n",
      "497.0        haciendo\n",
      "498.0            cosa\n",
      "499.0            unas\n",
      "500.0           serán\n",
      "Name: language:, Length: 500, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df[4])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import string\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "        and c in all_letters\n",
    "    )\n",
    "\n",
    "all_letters = string.ascii_letters + \" .,;'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la\n",
      "uuOOeeaa\n"
     ]
    }
   ],
   "source": [
    "print(unicodeToAscii(df[4].get(2)))\n",
    "print(unicodeToAscii(\"üüÖÖééäà\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from the pytorch tutorial\n",
    "n_letters = len(all_letters)\n",
    "\n",
    "# Find letter index from all_letters, e.g. \"a\" = 0\n",
    "def letterToIndex(letter):\n",
    "    return all_letters.find(letter)\n",
    "\n",
    "# Just for demonstration, turn a letter into a <1 x n_letters> Tensor\n",
    "def letterToTensor(letter):\n",
    "    tensor = torch.zeros(1, n_letters)\n",
    "    tensor[0][letterToIndex(letter)] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a line into a <line_length x 1 x n_letters>,\n",
    "# or an array of one-hot letter vectors\n",
    "def lineToTensor(line):\n",
    "    tensor = torch.zeros(len(line), 1, n_letters)\n",
    "    for li, letter in enumerate(line):\n",
    "        tensor[li][0][letterToIndex(letter)] = 1\n",
    "    return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "print(lineToTensor(df[3].get(25)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0                 die\n",
       "2.0                 und\n",
       "3.0                 der\n",
       "4.0                  in\n",
       "5.0                 das\n",
       "6.0                 den\n",
       "7.0                  zu\n",
       "8.0                 mit\n",
       "9.0                 ist\n",
       "10.0                von\n",
       "11.0                für\n",
       "12.0                auf\n",
       "13.0              nicht\n",
       "14.0                 es\n",
       "15.0                ein\n",
       "16.0                 im\n",
       "17.0               auch\n",
       "18.0               sich\n",
       "19.0                ich\n",
       "20.0                sie\n",
       "21.0               eine\n",
       "22.0                dem\n",
       "23.0                des\n",
       "24.0                 an\n",
       "25.0                als\n",
       "26.0             werden\n",
       "27.0                bei\n",
       "28.0               sind\n",
       "29.0               dass\n",
       "30.0                 so\n",
       "              ...      \n",
       "471.0             Abend\n",
       "472.0           mehrere\n",
       "473.0            damals\n",
       "474.0             Preis\n",
       "475.0            lernen\n",
       "476.0    beispielsweise\n",
       "477.0         einzelnen\n",
       "478.0            Gruppe\n",
       "479.0         weiterhin\n",
       "480.0             deine\n",
       "481.0           nämlich\n",
       "482.0              fand\n",
       "483.0         zumindest\n",
       "484.0              Juni\n",
       "485.0          Richtung\n",
       "486.0            gesagt\n",
       "487.0             hallo\n",
       "488.0      Gesellschaft\n",
       "489.0       mitarbeiter\n",
       "490.0           einigen\n",
       "491.0              lang\n",
       "492.0            schwer\n",
       "493.0              hoch\n",
       "494.0          Programm\n",
       "495.0             Augen\n",
       "496.0           welches\n",
       "497.0           dennoch\n",
       "498.0          geworden\n",
       "499.0             Rolle\n",
       "500.0              wert\n",
       "Name: language:, Length: 500, dtype: object"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = []\n",
    "fr = []\n",
    "ge = []\n",
    "it = []\n",
    "sp = []\n",
    "category_dict = {0: en, 1: fr, 2: ge, 3: it, 4: sp}\n",
    "\n",
    "def toCategory(num):\n",
    "    if (num == 0):\n",
    "        return 'english'\n",
    "    elif (num == 1):\n",
    "        return 'french'\n",
    "    elif (num == 2):\n",
    "        return 'german'\n",
    "    elif (num == 3):\n",
    "        return 'italian'\n",
    "    elif (num == 4):\n",
    "        return 'spanish'\n",
    "    else:\n",
    "        return 'category not handled'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    for j in range(500):\n",
    "        category_dict[i].append(unicodeToAscii(df[i].get(j+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "erhalten\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\ndf.append(pd.read_excel('words/english-word-list-total.xls'))\\ndf.append(pd.read_excel('words/french-word-list-total.xls'))\\ndf.append(pd.read_excel('words/german-word-list-total.xls'))\\ndf.append(pd.read_excel('words/italian-word-list-total.xls'))\\n#df.append(pd.read_excel('words/russian-word-list-total.xls'))\\ndf.append(pd.read_excel('words/spanish-word-list-total.xls'))\\n\""
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(category_dict[2][289])\n",
    "\n",
    "'''\n",
    "df.append(pd.read_excel('words/english-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/french-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/german-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/italian-word-list-total.xls'))\n",
    "#df.append(pd.read_excel('words/russian-word-list-total.xls'))\n",
    "df.append(pd.read_excel('words/spanish-word-list-total.xls'))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categories = 5\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "\n",
    "n_hidden = 138\n",
    "rnn = RNN(n_letters, n_hidden, n_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "tensor([[ 0.0166, -0.1953, -0.0858,  0.0423,  0.0065, -0.1618,  0.0430,  0.0223,\n",
      "          0.0463,  0.0242, -0.0567,  0.0334, -0.1433,  0.1061,  0.0459, -0.0327,\n",
      "          0.0931, -0.2018,  0.0877,  0.1011, -0.0028, -0.1127,  0.1291,  0.1176,\n",
      "          0.1174,  0.2298, -0.1222,  0.0707, -0.0832, -0.0040, -0.0027,  0.0633,\n",
      "         -0.0579, -0.0887, -0.1297,  0.1776, -0.0870,  0.1219,  0.1876, -0.1209,\n",
      "         -0.0890, -0.0676, -0.0791,  0.0011, -0.0238, -0.0582, -0.0484,  0.1104,\n",
      "          0.1432,  0.0176, -0.1086, -0.0266,  0.0016, -0.1478,  0.0065, -0.0431,\n",
      "         -0.0643, -0.0149,  0.1611,  0.0850, -0.1285,  0.1046, -0.2358, -0.0998,\n",
      "         -0.0298,  0.1324,  0.0155, -0.0512, -0.1857,  0.0396,  0.0534, -0.0962,\n",
      "         -0.0986, -0.0149, -0.0107, -0.0524,  0.1751,  0.0678, -0.0887,  0.0096,\n",
      "          0.1147,  0.0177, -0.1349,  0.1459, -0.0681, -0.0519,  0.2470,  0.0631,\n",
      "         -0.0545,  0.0880,  0.1298, -0.0403, -0.0561,  0.2561,  0.0412,  0.1409,\n",
      "          0.2678, -0.1420,  0.0077,  0.1616, -0.0686, -0.0016,  0.1020,  0.1747,\n",
      "         -0.0805,  0.0097,  0.0989,  0.1030,  0.0094,  0.0287,  0.0887,  0.1073,\n",
      "         -0.0678,  0.0042,  0.0882,  0.1679, -0.1481, -0.0539,  0.1204,  0.1814,\n",
      "         -0.0228, -0.0208,  0.1458,  0.1036,  0.1440,  0.0843, -0.1850,  0.1566,\n",
      "          0.1074, -0.0235, -0.0598,  0.2677,  0.2783, -0.0899, -0.0716, -0.1225,\n",
      "          0.0375,  0.1358]], grad_fn=<AddmmBackward>)\n",
      "tensor([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0.]])\n",
      "tensor([[ 0.1495, -0.1086,  0.0633,  0.0760,  0.1309,  0.2058,  0.1589, -0.2122,\n",
      "          0.0812, -0.0584,  0.1165,  0.0841, -0.0475,  0.1410,  0.0624,  0.0380,\n",
      "          0.0408, -0.3234, -0.2623, -0.0984,  0.0195, -0.1864,  0.2569, -0.1245,\n",
      "          0.1822, -0.0874, -0.0731, -0.0931, -0.0791, -0.2185, -0.1788,  0.1085,\n",
      "         -0.0654, -0.1520,  0.0660,  0.0004,  0.2216, -0.0903,  0.0260, -0.2032,\n",
      "          0.0794, -0.0523, -0.1066,  0.2117, -0.1188,  0.0038,  0.0078, -0.0678,\n",
      "          0.1140,  0.1991,  0.0182, -0.1285, -0.0677,  0.0442,  0.1178, -0.0295,\n",
      "         -0.0851,  0.0883, -0.0524,  0.2097,  0.0330,  0.1789, -0.0343, -0.0568,\n",
      "         -0.0684,  0.0518, -0.0926, -0.0278,  0.0589, -0.0513, -0.1228,  0.0825,\n",
      "         -0.1297,  0.1295, -0.1935, -0.1295, -0.0331, -0.1736, -0.1169, -0.0613,\n",
      "          0.0516,  0.0200,  0.2364, -0.0576,  0.0445, -0.0860, -0.1555,  0.1511,\n",
      "         -0.1049,  0.0136,  0.0382, -0.0729,  0.0679, -0.0922, -0.0733,  0.1202,\n",
      "          0.1757, -0.1533, -0.0718,  0.0881, -0.0918,  0.0807,  0.0384, -0.0140,\n",
      "         -0.0472,  0.0066, -0.0452, -0.0766,  0.0933, -0.0399,  0.0616,  0.0173,\n",
      "          0.0063,  0.1158, -0.1736, -0.2153, -0.0824, -0.0111, -0.0539,  0.0740,\n",
      "          0.2043,  0.0194,  0.0674, -0.0327, -0.0764,  0.0560,  0.0337,  0.0535,\n",
      "         -0.0377, -0.1980,  0.0416, -0.1182, -0.0162, -0.2185,  0.0170, -0.0379,\n",
      "         -0.0319, -0.2315]], grad_fn=<AddmmBackward>)\n",
      "tensor([[-0.6805, -1.8590, -1.4702, -3.1385, -2.7398]],\n",
      "       grad_fn=<LogSoftmaxBackward>)\n",
      "tensor([[ 0.3319,  0.1692,  0.0011, -0.3002,  0.3084, -0.0393, -0.1550,  0.0428,\n",
      "          0.0089,  0.1221, -0.1832, -0.0666,  0.1215,  0.3671, -0.1064,  0.0431,\n",
      "         -0.0322,  0.0222,  0.1743,  0.0562, -0.1761,  0.1131,  0.0828,  0.0369,\n",
      "         -0.1967, -0.0724,  0.1940,  0.0413,  0.1876, -0.1291,  0.2328, -0.2755,\n",
      "          0.1793,  0.0714, -0.1124,  0.0186, -0.2070,  0.0373, -0.0478,  0.2009,\n",
      "          0.0738, -0.0220, -0.2589,  0.0495, -0.1363,  0.0288, -0.0746, -0.3945,\n",
      "          0.3886,  0.0330,  0.0412, -0.1391,  0.0773,  0.1714, -0.0944,  0.0782,\n",
      "         -0.0920, -0.1125,  0.0756, -0.0633,  0.4592,  0.1279, -0.0915, -0.1568,\n",
      "          0.0608,  0.2169,  0.0630, -0.1457,  0.0298,  0.1892, -0.1635,  0.0601,\n",
      "          0.0375,  0.0881, -0.3063,  0.1324,  0.2983,  0.1709, -0.0226, -0.1286,\n",
      "          0.0698,  0.0864, -0.0623,  0.0610, -0.1234,  0.0013,  0.0453, -0.0976,\n",
      "          0.0959,  0.0139, -0.0829,  0.2011,  0.5283, -0.1107,  0.0895, -0.1699,\n",
      "         -0.1357, -0.0857, -0.0462, -0.0607,  0.0893,  0.2601, -0.1804,  0.0836,\n",
      "          0.2055, -0.0233, -0.0551,  0.2092, -0.3233, -0.0516,  0.0180,  0.0124,\n",
      "          0.1314,  0.1317,  0.0453, -0.1620,  0.2555, -0.1832, -0.0816, -0.2904,\n",
      "         -0.0065, -0.0461,  0.1394, -0.1647, -0.1383, -0.1268,  0.1406,  0.0864,\n",
      "          0.3100,  0.1026, -0.1777,  0.1203, -0.0697,  0.3799,  0.0118, -0.2362,\n",
      "          0.1812,  0.1679]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "hidden = torch.zeros(1, n_hidden)\n",
    "\n",
    "for line in lineToTensor('und'):\n",
    "    print(line)\n",
    "    print(hidden)\n",
    "    out, hid = rnn(line, hidden)\n",
    "    hidden = hid\n",
    "print(out)\n",
    "print(hid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1003"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict[0].insert(0, 'english')\n",
    "len(category_dict[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['english',\n",
       " 'english',\n",
       " 'the',\n",
       " 'and',\n",
       " 'to',\n",
       " 'of',\n",
       " 'a',\n",
       " 'in',\n",
       " 'is',\n",
       " 'that',\n",
       " 'for',\n",
       " 'I',\n",
       " 'you',\n",
       " 'it',\n",
       " 'with',\n",
       " 'on',\n",
       " 'as',\n",
       " 'are',\n",
       " 'be',\n",
       " 'this',\n",
       " 'was',\n",
       " 'have',\n",
       " 'or',\n",
       " 'at',\n",
       " 'not',\n",
       " 'your',\n",
       " 'from',\n",
       " 'we',\n",
       " 'by',\n",
       " 'will',\n",
       " 'can',\n",
       " 'but',\n",
       " 'they',\n",
       " 'an',\n",
       " 'he',\n",
       " 'all',\n",
       " 'has',\n",
       " 'if',\n",
       " 'their',\n",
       " 'one',\n",
       " 'do',\n",
       " 'more',\n",
       " \"n't\",\n",
       " 'my',\n",
       " 'his',\n",
       " 'so',\n",
       " 'there',\n",
       " 'about',\n",
       " 'which',\n",
       " 'when',\n",
       " 'what',\n",
       " 'out',\n",
       " 'up',\n",
       " 'our',\n",
       " 'who',\n",
       " 'also',\n",
       " 'had',\n",
       " 'time',\n",
       " 'some',\n",
       " 'would',\n",
       " 'were',\n",
       " 'like',\n",
       " 'been',\n",
       " 'just',\n",
       " 'her',\n",
       " 'new',\n",
       " 'other',\n",
       " 'them',\n",
       " 'she',\n",
       " 'people',\n",
       " 'these',\n",
       " 'no',\n",
       " 'get',\n",
       " 'how',\n",
       " 'me',\n",
       " 'into',\n",
       " 'than',\n",
       " 'only',\n",
       " 'its',\n",
       " 'most',\n",
       " 'may',\n",
       " 'any',\n",
       " 'many',\n",
       " 'make',\n",
       " 'then',\n",
       " 'well',\n",
       " 'first',\n",
       " 'very',\n",
       " 'over',\n",
       " 'now',\n",
       " 'could',\n",
       " 'after',\n",
       " 'even',\n",
       " 'because',\n",
       " 'us',\n",
       " 'said',\n",
       " 'good',\n",
       " 'way',\n",
       " 'two',\n",
       " 'should',\n",
       " 'work',\n",
       " 'use',\n",
       " 'through',\n",
       " 'see',\n",
       " 'know',\n",
       " 'did',\n",
       " 'much',\n",
       " 'where',\n",
       " 'years',\n",
       " 'need',\n",
       " 'him',\n",
       " 'back',\n",
       " 'such',\n",
       " 'those',\n",
       " 'being',\n",
       " 'day',\n",
       " 'take',\n",
       " 'while',\n",
       " 'here',\n",
       " 'before',\n",
       " 'does',\n",
       " 'great',\n",
       " 'year',\n",
       " 'go',\n",
       " 'help',\n",
       " 'want',\n",
       " 'really',\n",
       " 'think',\n",
       " 'best',\n",
       " 'life',\n",
       " 'each',\n",
       " 'made',\n",
       " 'right',\n",
       " 'world',\n",
       " 'business',\n",
       " 'home',\n",
       " 'own',\n",
       " 'down',\n",
       " 'still',\n",
       " 'used',\n",
       " 'find',\n",
       " 'around',\n",
       " 'going',\n",
       " 'every',\n",
       " 'both',\n",
       " 'last',\n",
       " 'off',\n",
       " 'too',\n",
       " 'same',\n",
       " 'information',\n",
       " 'little',\n",
       " 'another',\n",
       " 'look',\n",
       " 'few',\n",
       " 'long',\n",
       " 'part',\n",
       " 'since',\n",
       " 'things',\n",
       " 'place',\n",
       " 'am',\n",
       " 'between',\n",
       " 'during',\n",
       " 'different',\n",
       " 'must',\n",
       " 'come',\n",
       " 'using',\n",
       " 'however',\n",
       " 'without',\n",
       " 'high',\n",
       " 'why',\n",
       " 'something',\n",
       " 'online',\n",
       " 'system',\n",
       " 'better',\n",
       " 'three',\n",
       " 'never',\n",
       " 'always',\n",
       " 'love',\n",
       " 'say',\n",
       " 'might',\n",
       " 'next',\n",
       " 'company',\n",
       " 'state',\n",
       " 'number',\n",
       " 'again',\n",
       " 'free',\n",
       " 'lot',\n",
       " 'under',\n",
       " 'family',\n",
       " 'found',\n",
       " 'within',\n",
       " 'give',\n",
       " 'set',\n",
       " 'school',\n",
       " 'important',\n",
       " 'water',\n",
       " 'able',\n",
       " 'keep',\n",
       " 'got',\n",
       " 'sure',\n",
       " 'end',\n",
       " 'money',\n",
       " 'service',\n",
       " 'small',\n",
       " 'put',\n",
       " 'experience',\n",
       " 'having',\n",
       " 'once',\n",
       " 'available',\n",
       " 'health',\n",
       " 'support',\n",
       " 'often',\n",
       " 'including',\n",
       " 'days',\n",
       " 'away',\n",
       " 'old',\n",
       " 'area',\n",
       " 'feel',\n",
       " 'read',\n",
       " 'show',\n",
       " 'big',\n",
       " 'against',\n",
       " 'thing',\n",
       " 'order',\n",
       " 'program',\n",
       " 'though',\n",
       " 'city',\n",
       " 'group',\n",
       " 'services',\n",
       " 'site',\n",
       " 'making',\n",
       " 'course',\n",
       " 'point',\n",
       " 'children',\n",
       " 'times',\n",
       " 'team',\n",
       " 'game',\n",
       " 'along',\n",
       " 'let',\n",
       " 'house',\n",
       " 'today',\n",
       " 'body',\n",
       " 'working',\n",
       " 'case',\n",
       " 'man',\n",
       " 'real',\n",
       " 'provide',\n",
       " 'care',\n",
       " 'public',\n",
       " 'top',\n",
       " 'looking',\n",
       " 'several',\n",
       " 'start',\n",
       " 'less',\n",
       " 'process',\n",
       " 'become',\n",
       " 'actually',\n",
       " 'local',\n",
       " 'together',\n",
       " 'person',\n",
       " 'change',\n",
       " 'book',\n",
       " 'enough',\n",
       " 'getting',\n",
       " 'week',\n",
       " 'power',\n",
       " 'until',\n",
       " 'market',\n",
       " 'fact',\n",
       " 'god',\n",
       " 'food',\n",
       " 'students',\n",
       " 'full',\n",
       " 'women',\n",
       " 'community',\n",
       " 'name',\n",
       " 'second',\n",
       " 'data',\n",
       " 'government',\n",
       " 'says',\n",
       " 'others',\n",
       " 'ever',\n",
       " 'yet',\n",
       " 'research',\n",
       " 'done',\n",
       " 'left',\n",
       " 'far',\n",
       " 'large',\n",
       " 'called',\n",
       " 'doing',\n",
       " 'already',\n",
       " 'development',\n",
       " 'social',\n",
       " 'open',\n",
       " 'possible',\n",
       " 'side',\n",
       " 'play',\n",
       " 'means',\n",
       " 'needs',\n",
       " 'try',\n",
       " 'came',\n",
       " 'ca',\n",
       " 'based',\n",
       " 'hard',\n",
       " 'thought',\n",
       " 'products',\n",
       " 'national',\n",
       " 'quality',\n",
       " 'level',\n",
       " 'live',\n",
       " 'design',\n",
       " 'makes',\n",
       " 'project',\n",
       " 'line',\n",
       " 'night',\n",
       " 'least',\n",
       " 'whether',\n",
       " 'job',\n",
       " 'car',\n",
       " 'example',\n",
       " 'include',\n",
       " 'following',\n",
       " 'given',\n",
       " 'website',\n",
       " 'past',\n",
       " 'plan',\n",
       " 'offer',\n",
       " 'buy',\n",
       " 'call',\n",
       " 'went',\n",
       " 'simply',\n",
       " 'hand',\n",
       " 'music',\n",
       " 'easy',\n",
       " 'problem',\n",
       " 'men',\n",
       " 'country',\n",
       " 'took',\n",
       " 'four',\n",
       " 'members',\n",
       " 'form',\n",
       " 'personal',\n",
       " 'control',\n",
       " 'energy',\n",
       " 'room',\n",
       " 'head',\n",
       " 'pay',\n",
       " 'create',\n",
       " 'run',\n",
       " 'kind',\n",
       " 'credit',\n",
       " 'almost',\n",
       " 'believe',\n",
       " 'quite',\n",
       " 'mind',\n",
       " 'law',\n",
       " 'early',\n",
       " 'comes',\n",
       " 'states',\n",
       " 'usually',\n",
       " 'companies',\n",
       " 'web',\n",
       " 'taking',\n",
       " 'started',\n",
       " 'later',\n",
       " 'although',\n",
       " 'story',\n",
       " 'per',\n",
       " 'future',\n",
       " 'known',\n",
       " 'someone',\n",
       " 'across',\n",
       " 'rather',\n",
       " 'young',\n",
       " 'whole',\n",
       " 'special',\n",
       " 'everything',\n",
       " 'months',\n",
       " 'anything',\n",
       " 'training',\n",
       " 'url',\n",
       " 'bit',\n",
       " 'seen',\n",
       " 'product',\n",
       " 'american',\n",
       " 'please',\n",
       " 'management',\n",
       " 'cost',\n",
       " 'either',\n",
       " 'light',\n",
       " 'university',\n",
       " 'face',\n",
       " 'due',\n",
       " 'nothing',\n",
       " 'human',\n",
       " 'event',\n",
       " 'history',\n",
       " 'probably',\n",
       " 'friends',\n",
       " 'learn',\n",
       " 'current',\n",
       " 'tell',\n",
       " 'general',\n",
       " 'price',\n",
       " 'list',\n",
       " 'type',\n",
       " 'building',\n",
       " 'industry',\n",
       " 'bad',\n",
       " 'check',\n",
       " 'everyone',\n",
       " 'office',\n",
       " 'idea',\n",
       " 'internet',\n",
       " 'news',\n",
       " 'million',\n",
       " 'video',\n",
       " 'among',\n",
       " 'air',\n",
       " 'especially',\n",
       " 'told',\n",
       " 'results',\n",
       " 'post',\n",
       " 'hours',\n",
       " 'international',\n",
       " 'center',\n",
       " 'understand',\n",
       " 'above',\n",
       " 'addition',\n",
       " 'major',\n",
       " 'education',\n",
       " 'white',\n",
       " 'particular',\n",
       " 'problems',\n",
       " 'media',\n",
       " 'according',\n",
       " 'upon',\n",
       " 'page',\n",
       " 'continue',\n",
       " 'black',\n",
       " 'study',\n",
       " 'issues',\n",
       " 'inside',\n",
       " 'technology',\n",
       " 'five',\n",
       " 'value',\n",
       " 'further',\n",
       " 'access',\n",
       " 'reason',\n",
       " 'short',\n",
       " 'true',\n",
       " 'simple',\n",
       " 'natural',\n",
       " 'amount',\n",
       " 'search',\n",
       " 'result',\n",
       " 'taken',\n",
       " 'main',\n",
       " 'heart',\n",
       " 'space',\n",
       " 'financial',\n",
       " 'ago',\n",
       " 'trying',\n",
       " 'question',\n",
       " 'living',\n",
       " 'likely',\n",
       " 'interest',\n",
       " 'various',\n",
       " 'insurance',\n",
       " 'common',\n",
       " 'move',\n",
       " 'child',\n",
       " 'yourself',\n",
       " 'report',\n",
       " 'certain',\n",
       " 'share',\n",
       " 'single',\n",
       " 'close',\n",
       " 'instead',\n",
       " 'bring',\n",
       " 'works',\n",
       " 'age',\n",
       " 's',\n",
       " 'season',\n",
       " 'hope',\n",
       " 'coming',\n",
       " 'areas',\n",
       " 'ask',\n",
       " 'medical',\n",
       " 'low',\n",
       " 'games',\n",
       " 'turn',\n",
       " 'key',\n",
       " 'party',\n",
       " 'add',\n",
       " 'month',\n",
       " 'seems',\n",
       " 'view',\n",
       " 'fun',\n",
       " 'matter']"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict[0] = category_dict[0][:500]\n",
    "category_dict[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needed\n"
     ]
    }
   ],
   "source": [
    "category_dict[0].pop(0)\n",
    "category_dict[0].append('needed')\n",
    "print(category_dict[0][498])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_dict[0].insert(0, 'english')\n",
    "category_dict[0][0]\n",
    "len(category_dict[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict[1].insert(0, 'francais')\n",
    "category_dict[2].insert(0, 'deutsch')\n",
    "category_dict[3].insert(0, 'italiano')\n",
    "category_dict[4].insert(0, 'espanol')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    category_dict[i] = category_dict[i][:500]\n",
    "    assert len(category_dict[i]) == 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toIndex(df):\n",
    "    if df == 'english':\n",
    "        return 0\n",
    "    elif df == 'francais':\n",
    "        return 1\n",
    "    elif df == 'deutsch':\n",
    "        return 2\n",
    "    elif df == 'italiano':\n",
    "        return 3\n",
    "    elif df == 'espanol':\n",
    "        return 4\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "for i in range(5):\n",
    "    for j in range(500):\n",
    "        category_dict[i][j] = category_dict[i][j].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category = 4 / line = por / category tensor = tensor([4]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 1 / line = seront / category tensor = tensor([1]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 0 / line = this / category tensor = tensor([0]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 0 / line = days / category tensor = tensor([0]) / line tensor= tensor([[[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 2 / line = hier / category tensor = tensor([2]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 1 / line = nous / category tensor = tensor([1]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 1 / line = ca / category tensor = tensor([1]) / line tensor= tensor([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 0 / line = months / category tensor = tensor([0]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 3 / line = presente / category tensor = tensor([3]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n",
      "category = 2 / line = prozent / category tensor = tensor([2]) / line tensor= tensor([[[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "          0., 0., 0., 0., 0., 0.]]])\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def randomChoice(l):\n",
    "    return l[random.randint(0, len(l) - 1)]\n",
    "    # return r, l[r]\n",
    "\n",
    "def randomTrainingExample():\n",
    "    df = randomChoice(category_dict)\n",
    "    line = randomChoice(df)\n",
    "    category_nr = toIndex(df[0])\n",
    "    category_tensor = torch.tensor([category_nr], dtype=torch.long)\n",
    "    line_tensor = lineToTensor(line)\n",
    "    return category_nr, line, category_tensor, line_tensor\n",
    "\n",
    "for i in range(10):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    print('category =', category, '/ line =', line, '/ category tensor =', category_tensor, '/ line tensor=', line_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.007 # If you set this too high, it might explode. If too low, it might not learn\n",
    "\n",
    "def train(category_tensor, line_tensor):\n",
    "    hidden = rnn.initHidden()\n",
    "\n",
    "    rnn.zero_grad()\n",
    "\n",
    "    for i in range(line_tensor.size()[0]):\n",
    "        output, hidden = rnn(line_tensor[i], hidden)\n",
    "\n",
    "    loss = criterion(output, category_tensor)\n",
    "    loss.backward()\n",
    "\n",
    "    # Add parameters' gradients to their values, multiplied by learning rate\n",
    "    for p in rnn.parameters():\n",
    "        p.data.add_(-learning_rate, p.grad.data)\n",
    "\n",
    "    return output, loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('french', 1)\n"
     ]
    }
   ],
   "source": [
    "def categoryFromOutput(output):\n",
    "    top_n, top_i = output.topk(1)\n",
    "    category_i = top_i[0].item()\n",
    "    return toCategory(category_i), category_i\n",
    "\n",
    "print(categoryFromOutput(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "n_iters = 100000\n",
    "print_every = 5000\n",
    "#plot_every = 1000\n",
    "\n",
    "current_loss = 0\n",
    "all_losses = []\n",
    "\n",
    "def timeSince(since):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for iter in range(1, n_iters + 1):\n",
    "    category, line, category_tensor, line_tensor = randomTrainingExample()\n",
    "    output, loss = train(category_tensor, line_tensor)\n",
    "    current_loss += loss\n",
    "\n",
    "    # Print iter number, loss, name and guess\n",
    "    if iter % print_every == 0:\n",
    "        guess, guess_i = categoryFromOutput(output)\n",
    "        correct = '✓' if guess_i == category else '✗ (%s)' % category\n",
    "        print('%d %d%% (%s) %.4f %s / %s %s' % (iter, iter / n_iters * 100, timeSince(start), loss, line, guess, correct))\n",
    "        print(current_loss/print_every)\n",
    "        current_loss = 0\n",
    "\n",
    "    #Add current loss avg to list of losses\n",
    "    #if iter % plot_every == 0:\n",
    "    #    all_losses.append(current_loss / plot_every)\n",
    "    #    current_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
